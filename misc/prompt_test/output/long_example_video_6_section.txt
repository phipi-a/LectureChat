[32:09]
I didn't write the formula here but in the programming homework this entropy term or something similar is included.

[32:24]
So this is how the auto encoder looks like. So first we have your original data.

[32:32]
Then you pass it through this encoder function which is like multiplication by symmetric SV plus our compose with some RELU in linearity which gives you your vector S.

[32:37]
And then you map it into some decoder function w.

[32:51]
And then you get your reconstruction X hat. And it looks very much like neural network because you have basically one linear layer, one RELU layer and then one linear layer.

[33:06]
You have outputs with an input. And then basically you can define some objective and basically back propagates at least a reconstruction error.

[33:11]
You can for example learn the parameter v by back propagating through that network.

[33:17]
So taking this neural network view, you can then generalize the auto encoder.

[33:27]
You can create deep auto encoders.

[33:37]
And this was successfully applied in 2006 on some complex data sets.

[33:47]
And here the motivation is that a simple linear encoder function might not have to be sufficient to produce a meaningful source code.

[33:53]
Because sometimes you have like the data is complex and to extract meaningful dictionary elements that might have a bit of invariance for example you need to apply more transformation.

[34:07]
And so which means that now you can think of the encoder as a function g, which can be any function g, which is like one layer of multiple layers or something else.

[34:22]
And you have some decoder, which is some function f. And then this objective is consists of minimizing X minus composition of f and g plus some sparsity term that will apply to your intermediate layer basically plus some regularization which will basically force the decoder to basically not produce like you.

[34:47]
Yeah, to not produce large values if this was code is very small.

[34:59]
So yeah, here again this deep auto encoder can be trained using back propagation.

[35:08]
So you start at least for the reconstruction term you start here.

[35:13]
Then you back propagate the error reconstruction error through the network.

[35:21]
And then you can update the parameters of the decoder and encoder. For the sparsity term which is defined here, then in fact you would start here, define some additional error term here and then back propagate through this branch and you might actually update some parameters of the encoder.

[35:40]
And for the regularization, typically this will be some function on of this.

[35:45]
And you can again, as a, it's a simple regularization term which only involves the parameters or if it's more complex, you can again lose some back propagation.

[35:53]
But in practice, there are some efficient frameworks, for example, by torches or then torflow that allow you to not have to implement the back propagation yourself.

[36:11]
It uses automatic differentiation which basically requires you only to specify the objective or the forward pass.

[36:19]
Basically you need to write this and then you can basically call backward and it kind of pushes to create derivatives back into the network and collect the gradients wherever you need them.

[36:32]
So far we have mostly looked at autoencoder for the purpose of producing some spars or low dimensional representation which you might use for example for a storage solution.

[36:58]
But in practice, in fact it's also something that one has, that's in the paper on autoencoders was studied.

[37:14]
We would like to learn abstract representation. And what are these abstract representations? It's a presentation that might have built some level of invariance or extract some features that are useful for some prediction task of interest.

[37:26]
So for example in the case of face data, you might want to predict the age or male female.

[37:36]
And in that case, the representation should ideally only keep the factors in the data that are relevant for the task.

[37:52]
And room removed those that are irrelevant. So for example, you might want the factor as Z which is basically representation to be invariant to some more translation or rotation of the image because you know that's applying such transformation, should not change a prediction.

[38:15]
And for this, basically we not necessarily want to perfectly reconstruct any more.

[38:33]
We might want to, in fact, lose a bit of reconstruction accuracy for the purpose of building these different invariances.

[38:42]
So some examples that can trade a bit of reconstruction error in favor of something like in favor of building some invariance.

[38:54]
You have the bottleneck autoencoder.

[39:01]
So basically it's an autoencoder that has a middle layer which is as very low dimensionality.

[39:08]
And it will basically capture the essential element of the data and discard the rest.

[39:17]
And this is basically a generalization of the PCA ID.

[39:23]
In fact, there is a special connection between the autoencoder and PCA. It's that if you have a linear autoencoder, so basically there is no linearity in the middle layer.

[39:31]
With a bottleneck layer, H is more or equal to 2.

[39:38]
Then basically it will learn a solution that spans the PCA space.

[39:43]
And this was shown already a long time ago. Then there is another approach which is denosing autoencoder, where basically you will learn to reconstruct images that have been artificially corrupted by noise, for example, alitif, caution noise.

[40:00]
And it will show an example just after.

[40:06]
And by doing so, the representation will become less sensitive to noise components in the data.

[40:15]
So because it has to reconstruct, basically it has to remove noise from the image.

[40:22]
The representation will tend to learn direction in input space that are orthogonal to this noise.

[40:30]
And so as a result, you also simplify and filter some component of the data while expressing some others.

[40:38]
That might be more task relevance.

[40:47]
And the last approach, one more approach is the contractive autoencoder. And here the idea is to take your encoder function, G, and say, OK, I want my representation to be invariant to small variation in the data.

[41:01]
At least where my data is. And you can basically define this take your outputs, apply the gradient, you can get some Jacobian matrix.

[41:12]
And add it to the objective so that it penalizes a strong variation of the encoding function.

[41:22]
And this can also be done now with modern automatic differentiation software.

[41:32]
So I was mentioning the denosing autoencoder.

[41:43]
So this is basically how it works. There are different ways of syncing about it.

[41:48]
But a simpler one is to sync it off a simple autoencoder where you hard code a first layer of noise into your network.

[41:54]
So if you have the original data, then you have this green layer which can be cannot train with just adds noise.

[42:02]
So basically makes the task more complicated.

[42:11]
And then you have encoder and decoder that basically try to reconstruct your data.

[42:16]
And because you've added noise, you cannot reconstruct perfectly.

[42:22]
But ideally, you've kept some of the main information about the task.

[42:34]
Okay, so there is actually the limitation of this lossy reconstruction model that we've presented above.

[42:51]
Whether it's the bottleneck layer denosing autoencoder or conflicted autoencoders.

[42:58]
It is that also you are incorporating these constraints which basically try to build abstraction in addition to the reconstruction error.

[43:07]
It's still essentially learning a representation that optimizes the reconstruction error under these constraints.

[43:15]
So you still have basically this reconstruction term which will kind of force the network to preserve everything in the bottleneck.

[43:30]
So basically you have like a model that's try to solve to construct contradictory objectives, one which is to build some kind of robustness or invariance and at the same time trying to reconstruct the data.

[43:46]
And now we look at models that overcome this possible conflict and which really are designed to learn high level features and discard low level features.

[44:04]
And the first idea is to take an autoencoder and add some shortcut connections.

[44:15]
And what is the purpose of a shortcut connection? It basically to let some of the information flows directly from low level to the low level and just basically so that's not all the information has to pass through our vector Z.

[44:36]
And if we design things properly then Z would only preserve more abstract components of the image whereas the shortcut connection would rather focus on everything which is low level.

[44:51]
So the idea is that Z would kind of represent the abstract structure of the image.

[44:56]
For example like concept like whether you have like the different concepts that are composing this image and short connection would rather do something like fine alignments or fine rotation of the image to and then you kind of merge the two to reconstruct the data accurately.

[45:19]
And there are many models that have been proposed that implement this idea.

[45:30]
Maybe the very popular one is a unit that has been used for the task of segmentation.

[45:37]
But we also have this sort of connection. And one like two models that really explicitly try to build more abstract to presentation or at least this one in particular the ladder network has this structure and then also plug some supervised objective to basically get a better classification model.

[46:01]
And the deep model in both the machine which is some probabilistic model which also includes a short connection not explicitly like in any networks but via a special design of the probability function.

[46:25]
So this is the ladder network. So it's paper written in 2015 where basically the task is semi-supervised learning.

[46:38]
So basically it's a setting we have a lot of supervised data and very little supervised data.

[46:43]
And the idea in this paper was to have this auto encoder which is here shown by this end shape with some shortcut connection.

[46:54]
And then basically having and basically with this shortcut connection you will allow this top layer presentation to be more abstract.

[47:15]
And then the idea is that you have next to it some supervised models so it's turned out a feedforward network, a similar to the one that you've seen in machine learning one.

[47:25]
And in addition to the prediction cost you add this cost function at this level that can tie the representation to the one that have been built by the auto encoder.

[47:38]
And if you train the whole thing jointly then basically you get this abstractions that are built from a large amount of data and you try to synchronize them with abstractions that are required for the classifier.

[47:51]
And with this approach it's actually a very good model for super-supervised learning.

[48:00]
It produced state-of-the-art results on the task with strong manifold structure and particular amnesty where with very few labels you get very high accuracy.

[48:13]
So now we move a little bit away from the idea for auto encoders and discuss other approach for representation learning.

[48:27]
Always with the goal to try to build representations that are useful for some prediction task.

[48:36]
And one such approach which is very popular and very often used in practice it's a transfer learning.

[48:51]
And here instead of constructing the data we make use of an auxiliary task scheme.

[48:59]
And this auxiliary task would be something that you're not interested in but that for which we might have some labels for example you might have some data set of phases with some information that identifies these phases and might for example train a network that does that.

[49:22]
And if you think that the task would actually be the features that have been learned in some intermediate layer are useful for your real task which might be for example age prediction then you can take this part of the network and take away the rest of the blue parts and then connect add the two more layers here the green layers then say okay this is my new neural network composing five layers and I fine tune the whole thing on my task of interest.

[49:52]
And for transfer learning to work it is important that's the auxiliary task is related to the target task which means that it has to share some common features and also that it is more general.

[50:08]
So for example the task is more difficult or has more classes.

[50:15]
And transfer learning for example in the context of images so typically you will take you will start with the data set which is very large with a lot of labels in the context of image classification you have for example the image net data sets for specifically this ILS of your C bench mark which has millions of images and 1000 classes and with this we can build pretty general features and then you can transfer it on some more specific tasks which might contain let's say only two or three classes but that you might be interested in for your application.

[50:59]
Now we move to another approach which is basically between autoencoder and the transfer learning it's self-supervised learning and the idea is to generate if you don't have a noxular task like in the transfer learning scenario you can still generate an artificial task for learning the representation and the task is really complex enough so that's holding it produces useful features.

[51:37]
So one example of a auction task for image classification might be to say okay I will try to play a game I take an image I take a little square of the image out and then my prediction task is to impain the image.

[52:00]
So it's not exactly the same as reconstruct as an autoencoder because you really like take one part out and you just require to reconstruct the missing parts of the image and if the missing part of the image is large then somehow you expect that the network has to learn some, has to build some understanding of the image to be able to know what should be filled at the given missing parts of the image.

[52:31]
Another popular autoencoder task which is to learn to colorize an image.

[52:40]
So here we play another game which is that you take your RGB image like color images you make it artificially gray scale and then you train a neural network to predict the color image from the gray scale image.

[52:54]
So this is something that human can do pretty well.

[53:03]
So if you see for example this image you might say okay I recognize this fish and in fact I know that this fish is yellow and they are the head and the blue in the middle.

[53:16]
And for the network to be able to do the proper colorization it has to recognize the fish in the first place.

[53:24]
Same for the classes. There are other autoencoder tasks you can for example learn to predict whether two images are related or not.

[53:40]
Or you can try to solve some deep clustering tasks where you learn to map images into dense and well separated clusters.

[53:49]
And for all of these self supervised tasks no labels are needed.

[53:57]
So the only thing you need is creativity in setting up these self supervised tasks. And self supervised learning is a hot topic currently in machine learning.

[54:06]
Okay so now we look at the last topic for today which is how to represent transformation.

[54:15]
So here the idea is that we would like our network to predict not the input image itself but the transformation between two images or two data points.

[54:33]
And basically the question is can we represent the way to images are they form rather than the content of the image itself.

[54:41]
And in other words can we learn representation of transformation for example of translation which is invariant to the actual image content.

[54:56]
So we might for example want to learn a translation model on some handwritten digits.

[55:04]
And then without having ever seen some auto type of data for example real images we would like our translation model to work on these images as well.

[55:11]
And this is an example for and so basically we can do that with with techniques similar to auto encoders.

[55:21]
And this is illustration of the idea in the context of translation.

[55:27]
So translation are typically detected using a convolution. So you take your two images x and y then you convolve them and at some and basically you will see a peak here in basically your convolution.

[55:46]
That corresponds to the shift of the two images. So if x and y are two translated images basically the convolution will produce a maximum word to two function overlap.

[56:05]
And the same operation can be computed as a product. So you know from the four-year chart theorem that you can rewrite a convolution as mapping to four-year space of the two modalities of the two data points taking the products and then taking the inverse four-year transform and then you get basically your the convolution.

[56:28]
And now the idea is that we will learn f from the data and we will hard-code this product structure into our auto encoder.

[56:38]
So this is basically a toy case from translation scenario but then the idea will be that we might not know exactly what is the transform that produce a certain type of deformation which might be not translation will still use the same structure and then we will get different basis.

[57:04]
It will start with translation just to illustrate the concept.

[57:10]
So the auto encoder now consists of two parts and an encoder which produced the shift.

[57:18]
So basically this is what we've shown here but except that here we don't apply the four-year transform directly but just some encoding function that you have to learn and then you apply a dot product and then basically we get some shift representation.

[57:32]
So it's not exactly like a number of pixels but it's like a vector which encodes the shift and then the idea is to know how a second part which is a decoder which take your encoder inputs then apply the shift and multiply again and then have a decoding function that produces the translated image.

[58:01]
So now you can compose the two so basically you connect these two things, the shift and the shift and you have the whole neural network and you can train this thing into end by minimizing basically the min-square-error for example between the predicted and the true translation and this is basically a paper by Memes Advice at all from 2010 which uses a similar ID with this product network it's not exactly the one shown in the previous slide it's a bit more complicated it makes use of probabilistic model but basically this is the basis that is learned for the two modalities.

[58:46]
So here under left and the right you have two different basis function but what you observe first is that the average in the rather this is basically a 2D Fourier transform learned from the data which has been it has kind of recovered what we expected this to be to be learned by just learning to translate basically and what you observe for example is that this filter is the same as here just with a few pixels of translation this one again is horizontal and this one is shifted so basically the idea is that you kind of apply all these filters to your data points and then after you get a representation of the shift or basically various directions and then you map it back to the input space and you basically arrive at a translated version of your digit.

[59:51]
Now the interesting part is that now if you if you try to model other transformation for example rotation you keep the same architecture but you retrain on this translation and you look at the filters that you have learned and you learn all the types of basis functions so here we have these spiral-like filters and again you have the same structure so images are very similar but in fact they are similar up to a smaller rotation so actually the most amusing way to look at them is to take these images and just like scroll from one to the other repeatedly and you see these things moving a little bit.

[60:40]
Okay so that's basically the content for for today and with this we are done with the part of the course on component analysis so let's just recap what we have seen today similar to PCA, CCA and ICA, sparse coding and autoencoders, the representations from unsurprised data so we've been looking at mainly at unsurprised learning today.

[61:10]
Autoencoders can be trained quickly and they can be implemented as neural networks and therefore they can make use of all the neural network layers available for example convolution, pulling etc and autoencoders can serve different purposes for example they can produce compact sparse representations they can learn invariant representation they can learn abstract representation we've seen for example the ladder network which is also based on some kind of autoencoder and finally the last example I was describing I was presenting they can learn a transformation of the data rather than the data itself.

[62:02]
Okay so that's it for today thank you for your attention and yeah see you next week.

