Okay, so hello everyone. Today is the last lecture of the series of lecture on component analysis. So we've seen CCA, ICA, and today we look at something a bit more general, which is representation learning. So to set up the idea, basically you have an observation space which can be for example images, but this could also be sounds or any data you observe. And what you would like to do is to encode this data into some latent space, which is typically more abstract, which could be for example vectors. And then from this latent space we can go back to the observation space using some decoding. And there are many reasons why we would like to do this representation learning. And in some cases the latent space might be for example lower dimensional or more easy to express, and this could be used for compression or summarization. In some cases the goal of the latent space is to try to find a joint subspace between different modalities, and we have seen that in the lecture on CCA. Sometimes the purpose of the latent space is to extract factors that are basically a disentangling of the data, for example in the case of images, a decomposition in terms of edges or some independent components, and we've shown some examples last week when discussing ICA. And finally something we'll also look at today is that we can build this latent space to try to extract more high-level features that are more closely related to the task we would like to solve, which might be for example some image classification task or something else which is high-level. So far we've already surveyed a number of methods for representation learning, and the simplest one that we have already seen in the very beginning of machine learning 1 was a principal component analysis or PCA. And basically PCA tries to find a representation or a projection of the data where the variance is maximized, and the idea is that the main and first few principal components provide a good summarization of your data. We've also looked at canonical correlation analysis or CCA in machine learning 2 as well as ICA. And today we will continue with a few more of this analysis, and we will introduce, we'll present in particular sparse coding and autoencoders, and then we will look at a few more ideas, for example transfer learning, etc. So what is sparse coding? So sparse coding aims to represent the data in a way that in the latent representation many elements are zero. And the idea is that, so basically that's the definition of sparse, and contrary to dense, sparse basically is something which only contains a few elements and so the rest is assumed to be zero. And then the data can be reconstructed from this sparse code using a dictionary. And here you can see a cartoon on the left. And basically we can think of your source vector which is some kind of abstract vector of some dimensions, and each dimension is associated with some filter or some basis, and this set of basis forms a dictionary, let's call it W. And then basically if you take the non-zero element of the source vectors, so here those that are highlighted in red, and you sum them up, then basically you arrive at some reconstructed data which basically corresponds to the image represented by this source vector S. And sparse coding has several advantages compared to dense coding, and the first one is a low storage cost. So a vector with many zeros can be represented compactly. For example if you have a vector of this type, where the big red one would be like 12, here 0, here 7, and then 0, 0 again, you don't need to encode every individual entries of this vector. You can use a collection of this index value pairs, where basically you write the index and then the value. So for example here you have index 0, you have a value of 12, then you skip this one because it's 0, then you jump directly to index 2, so here index 2, you write 7, and then you skip index 3 and 4 because they are also 0. And so here, instead of having like 5 entries, you only have 4 of them, and if you have higher dimensional vectors that have higher sparsity, it becomes clearly more efficient. So that's one advantage of sparse coding, and a second advantage is that it is actually more interpretable. So it's nice to be able to think of your data as a combination of few factors, and then you can think of basically just the data being like the reconstruction or the superposition of a few dictionary elements. So the first model we will look at is linear sparse coding. And here we will assume that we have a vector, basically a data vector x, which is some abstract d-dimensional space. So if we think of an image, we just take the image matrix, then we flatten it and we make a vector out of it. So this image, maybe it's like 32 times 32, or if you flatten it you get a vector of size 1024. And then we'll have our source code, which is again a vector in Rh with h different than d. And usually in PCA, h was much smaller than d. But in sparse coding, it does not have to be much smaller, it can be the same size or also larger. In that case, we talk about an overcomplete representation. And this doesn't really cause a problem, because many of these elements will be zero. So if you think of trying to describe, for example, this in terms of an image in terms of words, there might be many more words that would potentially describe an image than the number of pixels that form the image. So it's not a problem to have h larger than d. And in fact, we will typically prefer in the case of sparse coding to have large high dimensional source vectors. Okay, then the dictionary W. So this is basically a matrix of size d times h. Or it can sometimes be h times d, it depends on how you write the formulas. I think in the programming exercise, it's of size h times d. But basically, it's the collection of all the basis in input space. So basically, each basis is a vector of dimension d. And if you just stack them, you get a matrix of size d times h. And linear sparse coding reconstructs approximately the data from the source code, basically using this matrix multiplication. So you take s, which is a vector, rh, then you multiply it by this matrix, and then you get some x hat, where x hat is the reconstructed data. And this is basically a matrix multiplication. This is actually the dense formulation, but because many of these vectors, elements are sparse, in practice, it's inefficient. Or we can basically get a more efficient formulation by basically summing over the non-zero indices. And here, basically, by this we mean the is column of the matrix W, that we then multiply with the entry s i of the source vector, and we just sum them up. So if we have like, only like 1% of the source vectors that are, let's say if you have like a source vector of size 200, and only two of these elements are active, instead of having this big matrix multiplication, you just have to sum over two elements, and apply basically sum over two vectors. So that's basically the way the model is defined. And now the big question will be how do we learn the dictionary W, as well as the sparse code s1 until sn, if we have some data set x1 until xn. So we know neither of them, we don't know the dictionary priori, we'd like to learn it. And the sparse codes, we also need to learn them. So we'll first start with the L0 formulation, which is like, theoretically optimal, but which is not practically, which cannot be optimized practically, or at least not with simple methods. And we will have basically our data set, which is composed of our collection of vectors in Rd. Then we will have our associated sources, which is basically a collection of vectors in Rh. And then we will finally have W, which is our dictionary that reconstruct the data from the sources. And now we will try to implement the following objective, which is basically the mean square error between the data and the reconstruction, right, it's the reconstruction error. And in addition, we want to reconstruct, subject to the constraint that the source vector is sparse. And one way to implement that is to apply the zero norm with norm in quotes, because it's not a norm actually, but it's just basically generalization of the p-norm with p equal to zero. And what this zero norm does is that it counts the number of non-zero elements in the vector. So here, for example, if you have this vector, the zero norm would be simply two. And so if you minimize that, you have an objective which tries to reconstruct and at the same time try to create source vectors that are as sparse as possible. So if we could optimize this, that would be very good. But the problem is that the zero norm is non-convex and not differentiable, which makes it hard to optimize. So one idea is to apply what we call L1 relaxation. And the idea is to replace the zero norm by one norm. And one norm is basically sometimes called the Manhattan distance between SE and the origin. And here, basically, the only thing that changes from the previous optimization problem is that we have a one here. And the nice thing about the L1 norm, the one norm, is that it's convex. And also it is differentiable almost everywhere except at the hinges of the norm. But most likely, we will never be exactly at that point. So basically, now we have a convex optimization problem. We can, for example, solve it by gradient descent or by something more efficient. But there is actually a problem with this. OK, further is a limitation, is that we are not optimizing the exact L0 formulation of sparse coding. So we will not get the maximally, like optimally sparse solution, but something like a sparse solution. In practice, it will be most of the time sufficient. But yes, there are actually some approach to sparse coding that try to go beyond L1 norm and try to use like something non-convex, which is closer to the L0, to the zero norm. And there is another problem. And this one is actually more severe, is that if you scale up W and scale down Si accordingly, then this term does not change. Let's say if you multiply W by 2 and you divide Si by 2, this doesn't change. But this thing decreases by a factor of 2. So you can just keep scaling up and scaling down these two terms. And at the end, the sparsity term becomes almost zero. And essentially, you get a sparsity term which is not effective. So you can always create an optimally reconstructing solution, optimally sparse, by choosing a dictionary with very large weights. So we don't want that, because we want to actually have some effective sparsity. And for this, we will add a regularization term on the weights of the dictionary, which can be, for example, simply eta times the Frobenius norm of the dictionary W. So basically, what is the Frobenius norm? Square is the sum of all the square values of the matrix. There are different formulations, a different way to incorporate this constraint. Here, it's an additive term, but sometimes you can also see in the literature this constraint enforced as some actual constraint. The norm of the weights have to be smaller than some parameter C. So there are different formulations. But in any case, this serves to address the problem of making the sparsity term effective. OK. So now to illustrate what sparse coding does, we will look at the toy example, where you have some data sets of n data points in error 2. And then we will perform source coding with sources in error 3. And after optimization, most of the data points will be sparse in source space and align with the coordinate system. So basically, this is like a simulation. So here, we have taken some input data. We have fixed the dictionary to be optimal. And then we run some gradient descent over the sources. And this is what we find. So basically, if we apply the L1 norm, then the sources naturally converge towards either the origin or not the origin, but as close as possible to the canonical coordinate system. So we get this kind of cloud of data points. And then if you project back on the dictionary, then basically this thing maps there, this thing maps there, and this one maps there. And then basically, we get some reconstruction of our input data, which is not optimal, but still quite acceptable. So for example, most of these points will map to this region of the input space. So probably the one that are close to this line will map exactly there. And only the few points that are far away will be left over there. And yeah, then there's the question of the parameter lambda. So if you remember in this optimization problem, there is a factor lambda, which basically determines how strongly we apply the sparsity penalty. And it's interesting to see the effect of the parameter lambda. So if you use a small parameter lambda, then basically nothing happens. So you just have a perfect reconstruction and no sparsity at all. If you increase lambda a little bit, then you start to see that a few points start to agglomerate in the region of sparsity. So where only one of the elements is zero, one element is non-zero. If you keep increasing lambda, then most of the points starts to align with the canonical coordinate system in the source space and then project back onto this direction of the dictionary. And if you just keep increasing lambda, then